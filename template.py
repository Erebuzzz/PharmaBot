# -*- coding: utf-8 -*-
"""Copy of Session 2 - RAG in Action.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1acp9aqy73FPBI6W-zeZRTkRsmLH2p6F9

# Initialisation
"""

import nltk
from nltk import word_tokenize
from nltk import sent_tokenize

from nltk import pos_tag
from nltk.tag.mapping import map_tag

import numpy as np

nltk.download('all', quiet=True)

def get_upos_tags(text: str):
    """
    Tokenizes the text and returns (word, UPOS) pairs.
    Example UPOS tags: NOUN, VERB, ADJ, DET, ADP, etc.
    """
    word_list = word_tokenize(text)
    treebank_tags = pos_tag(word_list)
    upos_tags = [(word, map_tag('en-ptb', 'universal', tag)) for word, tag in treebank_tags]
    return upos_tags

# Install the Sentence Transformer library
!pip install --quiet --upgrade sentence-transformers

import os
import logging

# Suppress Hugging Face / Transformers logs
os.environ["TRANSFORMERS_VERBOSITY"] = "error"  # only show errors

logging.getLogger("transformers").setLevel(logging.ERROR)
logging.getLogger("sentence_transformers").setLevel(logging.ERROR)

from sentence_transformers import SentenceTransformer, util

# Load model silently
model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

def get_embedding(text):
    return model.encode(text,convert_to_tensor=True)

def find_text_similarity(text1, text2):
    return round(float(util.pytorch_cos_sim(get_embedding(text1), get_embedding(text2))[0][0]), 4)

def find_embedding_similarity(emb1, emb2):
    return round(float(util.pytorch_cos_sim(emb1, emb2)[0][0]), 4)

def create_embedding_list(text_list):
    return [get_embedding(text) for text in text_list]

def find_answer(query, text_list, embedding_list, threshold=0.5):
    query_embedding = get_embedding(query)
    sim_list = [find_embedding_similarity(query_embedding, emb) for emb in embedding_list]
    print("Similarity values between query and all sentences in the list:")
    print(sim_list)
    print("")

    max_sim = max(sim_list)
    if max_sim < threshold:
        return f"No relevant answer found above the similarity threshold of {threshold}."
    else:
        return text_list[sim_list.index(max_sim)]

import requests
from bs4 import BeautifulSoup

def fetch_wikipedia_html(url: str) -> str:
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                      "AppleWebKit/537.36 (KHTML, like Gecko) "
                      "Chrome/91.0.4472.114 Safari/537.36"
    }
    response = requests.get(url, headers=headers)
    response.raise_for_status()
    return response.text

def extract_paragraphs(html: str) -> list[str]:
    soup = BeautifulSoup(html, "html.parser")

    # Main content of Wikipedia pages is inside <div id="mw-content-text">
    content_div = soup.find("div", {"id": "mw-content-text"})
    paragraphs = []

    if content_div:
        for p in content_div.find_all("p"):
            text = p.get_text().strip()
            if text:  # ignore empty or citation-only paragraphs
                paragraphs.append(text)
    return paragraphs

def find_answer_top_k(query, text_list, embedding_list, k=2):
    query_embedding = get_embedding(query)
    sim_list = [find_embedding_similarity(query_embedding, emb) for emb in embedding_list]

    # Get indices of top-k similarities (sorted descending)
    top_k_indices = np.argsort(sim_list)[::-1][:k]

    # Return top-k texts as a list
    return [text_list[i] for i in top_k_indices]

from openai import OpenAI

from dotenv import load_dotenv
import os
load_dotenv("api_key.env")
api_key = os.getenv('api_key')

# 1. Visit : https://openrouter.ai/
# 2. Login with your Gmail ID and create a new API Key (top right menu)
# 3. Create api_key.env file in the same folder as your code
# 4. Put the api_key there. Just write one line (use your own API Key, the one below wont work):
# api_key=sk-or-v1-aasdewr34asdc0r31oweijf

def get_ai_response(prompt):
  client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key="api_key",
  )

  completion = client.chat.completions.create(
      model="deepseek/deepseek-r1-0528:free",
    messages=[
      {
        "role": "user",
        "content": prompt
      }
    ]
  )

  return completion.choices[0].message.content

"""# NLP Tools"""

text = "I am learning Large Language Models. I will then build a cool application using it."

# Sentence Tokenization
# Tokenization is basically breaking a piece of text
# into smaller units.
sent_tokenize(text)

# Word Tokenization
word_tokenize(text)

# What AI does is sub-word tokenization.

# POS Tagging (Parts of Speech)
print(get_upos_tags(text))

# One application is sentiment analysis.

# Named Entity Recognition [NER]
# classify words into categories..
# Company Name, Person Name, Brand, Location

"""# LLM Tools"""

text = "I am a human being."
get_embedding(text)

text1 = "I am a human being."
text2 = "What are you doing?"
find_text_similarity(text1, text2)
# So this value ranges between -1 to +1.

# ONLY TEXT DOES NOT HAVE A NATURAL NUMERIC REPRESENTATION.



"""# RAG : Retrieval Augmented Generation"""

query = "whats special about Amazon?"

get_ai_response(query)

text = """
The Amazon rainforest is the largest tropical rainforest in the world,
spanning across nine countries in South America.
It is home to an incredible diversity of wildlife,
including jaguars, toucans, and countless species of insects.
The rainforest also plays a crucial role in regulating the Earth's climate
by absorbing large amounts of carbon dioxide.
Unfortunately, deforestation and illegal logging have been
threatening this vital ecosystem for decades.
Conservation efforts are underway, but sustainable practices need to be
adopted globally to preserve this natural treasure for future generations."""

text_list = sent_tokenize(text)

# Each item in this list is called a "chunk"
print(text_list)

for item in text_list:
    print(item)
    print("")

emb_list = create_embedding_list(text_list)

find_answer(query, text_list, emb_list)

query = "What is needed to preserve nature?"
find_answer(query, text_list, emb_list)

query = "How do LLMs work?"
find_answer(query, text_list, emb_list)



"""**Issues:**

1. Is the most similar text always the best answer?



2. What if the text list you have does not have the actual answer?


3. What if the answer is spread over several text chunks?


4. How to deal with PDFs and other data sources?

# Smarter RAG System
"""

url = "https://en.wikipedia.org/wiki/Amazon_rainforest"

html = fetch_wikipedia_html(url)
paragraphs = extract_paragraphs(html)

print(f"Extracted {len(paragraphs)} paragraphs.")

paragraphs[0]

# For actual RAG pipeline, a lot of text cleaning is also required.

emb_list = create_embedding_list(paragraphs)

query = "What is needed to preserve nature?"
context_list = find_answer_top_k(query, paragraphs, emb_list, k=2)
for context in context_list:
    print(context)
    print("")

context = " ".join(context_list)

# AI is like a dumb intern.
prompt = f"""

INSTRUCTIONS:
- Answer from the given context only.
- Think about the query carefully.

User Query: {query}.

Context: {context}
"""

get_ai_response(prompt)

"""Think:

How can we improve the AI response?
"""



"""# Chatting with PDFs

References:

https://unstructured.io

https://www.youtube.com/watch?v=uLrReyH5cu0

https://docs.unstructured.io/open-source/core-functionality/chunking
"""

!pip install -Uq "unstructured[all-docs]" pillow lxml pillow

!apt-get install -y poppler-utils

import requests

# Download the PDF locally
url = "https://arxiv.org/pdf/2008.07779"
file_path = "paper.pdf"
with open(file_path, "wb") as f:
    f.write(requests.get(url).content)

from unstructured.partition.pdf import partition_pdf

chunks = partition_pdf(
    filename=file_path,
    infer_table_structure=True,            # extract tables
    strategy="hi_res",                     # mandatory to infer tables

    extract_image_block_types=["Image", "Table"],   # Add 'Table' to list to extract image of tables
    # image_output_dir_path=output_path,   # if None, images and tables will saved in base64

    extract_image_block_to_payload=True,   # if true, will extract base64 for API usage

    chunking_strategy="by_title",          # or 'basic'
    max_characters=10000,                  # defaults to 500
    combine_text_under_n_chars=2000,       # defaults to 0
    new_after_n_chars=6000,

    # extract_images_in_pdf=True,          # deprecated
)

chunks[0].__dict__



"""Multi-modal AI

Its an adaptation of the original transformer model to take care of non-tex data inputs.

For processing PDFs, we need to extract text, tables and images. Process text like the normal RAG pipeline. But for tables and images, we need to feed it to multi-modal AI to get a text summary of the contents. And then use this text summary in our RAG pipeline.

# Extract Tables
"""

chunk_tables = []
for chunk in chunks:
    elements = chunk.metadata.orig_elements
    chunk_tables.extend([el for el in elements if 'Table' in str(type(el))])
chunk_tables

chunk_tables[0].__dict__

import base64
from io import BytesIO
from PIL import Image

img_data = base64.b64decode(chunk_tables[0].metadata.image_base64)
img = Image.open(BytesIO(img_data))
display(img)

"""# Extract Images"""

chunk_images = []
for chunk in chunks:
    elements = chunk.metadata.orig_elements
    chunk_images.extend([el for el in elements if 'Image' in str(type(el))])
chunk_images

# Get the images from the CompositeElement objects
def get_images_base64(chunks):
    images_b64 = []
    for chunk in chunks:
        if "CompositeElement" in str(type(chunk)):
            chunk_els = chunk.metadata.orig_elements
            for el in chunk_els:
                if "Image" in str(type(el)):
                    images_b64.append(el.metadata.image_base64)
    return images_b64

images = get_images_base64(chunks)

import base64
from IPython.display import Image, display

def display_base64_image(base64_code):
    # Decode the base64 string to binary
    image_data = base64.b64decode(base64_code)
    # Display the image
    display(Image(data=image_data))

display_base64_image(images[0])

# AI models are for answering generic questions.

# We use RAG because AI models do not have all the answers.


# Why do we need AI agents?

# When we need to do reasoning.
# Lets say a company has many tasks which can be automated.
# And you have build AI systems for all these tasks.
# And I need AI agents to reason and plan and execute suitable AI pipelines.
# There are two approaches to this.
# One is fully automated (Eg. CrewAI)
# One is using our own algo (Eg. LangGraph)